#!/usr/bin/env python3
"""
NBA Alternate Spreads Collector v2 (with Incremental Update)
- ê³¼ê±° ê²½ê¸°ì˜ Alternate Spreads ìˆ˜ì§‘ (Favorites: -12.5 ~ -2.5, Underdogs: +2.5 ~ +12.5)
- The-Odds-APIì˜ /events/{eventId}/odds ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
- ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì› (ê¸°ì¡´ ë°ì´í„°ì— ìƒˆ ë‚ ì§œë§Œ ì¶”ê°€)
"""

import requests
import json
import logging
import pytz
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import time


class NBAAlternateSpreadsCollector:
    """NBA Alternate Spreads ìˆ˜ì§‘ê¸° (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""
    
    # NBA íŒ€ëª… ë§¤í•‘ (ì „ì²´ ì´ë¦„ -> ì•½ì)
    NBA_TEAM_ABBREV = {
        'Atlanta Hawks': 'ATL',
        'Boston Celtics': 'BOS',
        'Brooklyn Nets': 'BKN',
        'Charlotte Hornets': 'CHA',
        'Chicago Bulls': 'CHI',
        'Cleveland Cavaliers': 'CLE',
        'Dallas Mavericks': 'DAL',
        'Denver Nuggets': 'DEN',
        'Detroit Pistons': 'DET',
        'Golden State Warriors': 'GSW',
        'Houston Rockets': 'HOU',
        'Indiana Pacers': 'IND',
        'LA Clippers': 'LAC',
        'Los Angeles Clippers': 'LAC',
        'Los Angeles Lakers': 'LAL',
        'Memphis Grizzlies': 'MEM',
        'Miami Heat': 'MIA',
        'Milwaukee Bucks': 'MIL',
        'Minnesota Timberwolves': 'MIN',
        'New Orleans Pelicans': 'NOP',
        'New York Knicks': 'NYK',
        'Oklahoma City Thunder': 'OKC',
        'Orlando Magic': 'ORL',
        'Philadelphia 76ers': 'PHI',
        'Phoenix Suns': 'PHX',
        'Portland Trail Blazers': 'POR',
        'Sacramento Kings': 'SAC',
        'San Antonio Spurs': 'SAS',
        'Toronto Raptors': 'TOR',
        'Utah Jazz': 'UTA',
        'Washington Wizards': 'WAS'
    }
    
    # ìˆ˜ì§‘í•  Spread êµ¬ê°„ (Favorites: ìŒìˆ˜, Underdogs: ì–‘ìˆ˜)
    TARGET_SPREADS = [
        -18.5, -17.5, -16.5, -15.5, -14.5, -13.5, -12.5, -11.5, -10.5, -9.5, -8.5, -7.5, -6.5, -5.5, -4.5, -3.5, -2.5,  # Favorites (17ê°œ)
        +2.5, +3.5, +4.5, +5.5, +6.5, +7.5, +8.5, +9.5, +10.5, +11.5, +12.5, +13.5, +14.5, +15.5, +16.5, +17.5, +18.5   # Underdogs (17ê°œ)
    ]
    
    def __init__(self, api_key: str, days_back: int = 2, incremental: bool = True):
        """
        Args:
            api_key: The-Odds-API í‚¤
            days_back: ê³¼ê±° ë©°ì¹ ì¹˜ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ë³¸ 2ì¼)
            incremental: ì¦ë¶„ ì—…ë°ì´íŠ¸ ëª¨ë“œ (True: ìƒˆ ë‚ ì§œë§Œ, False: ì „ì²´ ì¬ìˆ˜ì§‘)
        """
        self.api_key = api_key
        self.base_url = "https://api.the-odds-api.com/v4"
        self.sport = "basketball_nba"
        self.days_back = days_back
        self.incremental = incremental
        
        # ì‹œê°„ëŒ€ ì„¤ì • (ë™ë¶€ì‹œê°„)
        self.eastern_tz = pytz.timezone('US/Eastern')
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.project_root = Path(__file__).parent
        self.spreads_dir = self.project_root / "data" / "alternate_spreads"
        self.spreads_dir.mkdir(parents=True, exist_ok=True)
        
        # ë§ˆìŠ¤í„° íŒŒì¼ ê²½ë¡œ
        self.master_file = self.spreads_dir / "nba_alternate_spreads_fanduel_master.json"
        
        # ë¡œê¹… ì„¤ì •
        self.logger = self._setup_logging()
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger("NBAAlternateSpreadsCollector")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)
        
        return logger
    
    def load_master_file(self) -> List[Dict]:
        """ë§ˆìŠ¤í„° íŒŒì¼ ë¡œë“œ"""
        if not self.master_file.exists():
            self.logger.info("ğŸ“‚ No existing master file found. Starting fresh.")
            return []
        
        try:
            with open(self.master_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"ğŸ“‚ Loaded {len(data)} existing spread options from master file")
            return data
        except Exception as e:
            self.logger.warning(f"âš ï¸  Error loading master file: {e}. Starting fresh.")
            return []
    
    def get_latest_date_from_master(self) -> Optional[str]:
        """ë§ˆìŠ¤í„° íŒŒì¼ì—ì„œ ê°€ì¥ ìµœê·¼ ë‚ ì§œ ì¶”ì¶œ"""
        existing_data = self.load_master_file()
        if not existing_data:
            return None
        
        # ëª¨ë“  ë‚ ì§œ ì¶”ì¶œ í›„ ìµœì‹  ë‚ ì§œ ë°˜í™˜
        dates = [item['date'] for item in existing_data if 'date' in item]
        if dates:
            latest = max(dates)
            self.logger.info(f"ğŸ“… Latest date in master file: {latest}")
            return latest
        return None
    
    def get_target_dates(self) -> List[str]:
        """
        ì˜¤ëŠ˜ ê¸°ì¤€ ê³¼ê±° Nì¼ì˜ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì¦ë¶„ ëª¨ë“œ ê³ ë ¤)
        âš ï¸ ì¤‘ìš”: ì–´ì œê¹Œì§€ë§Œ ìˆ˜ì§‘ (ê²½ê¸° ê²°ê³¼ê°€ í™•ì •ëœ ê³¼ê±° ë°ì´í„°ë§Œ ëŒ€ìƒ)
        
        Returns:
            ë‚ ì§œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ì˜¤ë˜ëœ ë‚ ì§œë¶€í„°) ['2025-11-11', '2025-11-12']
        """
        # ë™ë¶€ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œ ê³„ì‚°
        today_et = datetime.now(self.eastern_tz).date()
        
        if self.incremental:
            # ì¦ë¶„ ëª¨ë“œ: ë§ˆìŠ¤í„° íŒŒì¼ì˜ ìµœì‹  ë‚ ì§œ ì´í›„ë§Œ ìˆ˜ì§‘
            latest_date_str = self.get_latest_date_from_master()
            if latest_date_str:
                latest_date = datetime.strptime(latest_date_str, '%Y-%m-%d').date()
                # ìµœì‹  ë‚ ì§œ ë‹¤ìŒ ë‚ ë¶€í„° ì–´ì œê¹Œì§€ (ë™ë¶€ì‹œê°„ ê¸°ì¤€)
                start_date = latest_date + timedelta(days=1)
                yesterday_et = today_et - timedelta(days=1)
                
                if start_date > yesterday_et:
                    self.logger.info("âœ… Master file is up to date. No new dates to collect.")
                    return []
                
                dates = []
                current = start_date
                while current <= yesterday_et:
                    dates.append(current.strftime('%Y-%m-%d'))
                    current += timedelta(days=1)
                
                self.logger.info(f"ğŸ”„ Incremental mode: Collecting {len(dates)} new date(s)")
                return dates
        
        # ì „ì²´ ìˆ˜ì§‘ ëª¨ë“œ ë˜ëŠ” ë§ˆìŠ¤í„° íŒŒì¼ ì—†ìŒ (ë™ë¶€ì‹œê°„ ê¸°ì¤€)
        # âš ï¸ ì¤‘ìš”: ì–´ì œê¹Œì§€ë§Œ ìˆ˜ì§‘ (ê²½ê¸° ê²°ê³¼ê°€ ë‚˜ì˜¨ ê³¼ê±° ë°ì´í„°ë§Œ)
        yesterday_et = today_et - timedelta(days=1)
        dates = []
        for i in range(self.days_back, 0, -1):
            target_date = today_et - timedelta(days=i)
            # ì–´ì œ ì´ì „ ë‚ ì§œë§Œ í¬í•¨
            if target_date <= yesterday_et:
                dates.append(target_date.strftime('%Y-%m-%d'))
        
        return dates
    
    def fetch_game_list_for_date(self, date_str: str) -> Optional[List[Dict]]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ê²½ê¸° ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (game_id ìˆ˜ì§‘ìš©)
        
        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD)
            
        Returns:
            ê²½ê¸° ë¦¬ìŠ¤íŠ¸ (game_id, teams, commence_time í¬í•¨)
        """
        url = f"{self.base_url}/historical/sports/{self.sport}/odds"
        
        params = {
            'apiKey': self.api_key,
            'regions': 'us',
            'markets': 'h2h',  # h2hë¡œ ê²½ê¸° ë¦¬ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜´
            'oddsFormat': 'american',
            'date': f"{date_str}T12:00:00Z",
            'bookmakers': 'fanduel'
        }
        
        try:
            self.logger.debug(f"ğŸ“¡ Fetching game list for {date_str}...")
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
            if isinstance(data, dict) and 'data' in data:
                games = data['data']
            elif isinstance(data, list):
                games = data
            else:
                self.logger.warning(f"âš ï¸  Unexpected response structure for {date_str}")
                return []
            
            # ê²Œì„ ì •ë³´ ì¶”ì¶œ
            game_list = []
            for game in games:
                game_list.append({
                    'id': game.get('id'),
                    'home_team': game.get('home_team'),
                    'away_team': game.get('away_team'),
                    'commence_time': game.get('commence_time')
                })
            
            return game_list
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ Error fetching game list for {date_str}: {e}")
            return None
    
    def fetch_alternate_spreads_for_game(self, game_id: str, date_str: str, 
                                        home_team_full: str, away_team_full: str,
                                        commence_time_utc: str) -> List[Dict]:
        """
        íŠ¹ì • ê²½ê¸°ì˜ Alternate Spreads ê°€ì ¸ì˜¤ê¸°
        
        Args:
            game_id: ê²½ê¸° ID
            date_str: ë‚ ì§œ ë¬¸ìì—´
            home_team_full: í™ˆíŒ€ ì „ì²´ ì´ë¦„
            away_team_full: ì›ì •íŒ€ ì „ì²´ ì´ë¦„
            commence_time_utc: ê²½ê¸° ì‹œì‘ ì‹œê°„ (UTC)
            
        Returns:
            ì²˜ë¦¬ëœ spread ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        # Historical API ì‚¬ìš© (ê³¼ê±° ê²½ê¸°)
        url = f"{self.base_url}/historical/sports/{self.sport}/events/{game_id}/odds"
        
        params = {
            'apiKey': self.api_key,
            'regions': 'us',
            'markets': 'alternate_spreads',
            'oddsFormat': 'american',
            'date': f"{date_str}T12:00:00Z",  # Historical APIì—ëŠ” date íŒŒë¼ë¯¸í„° í•„ìš”
            'bookmakers': 'fanduel'
        }
        
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # ì‘ë‹µ êµ¬ì¡°: {'data': {...}}
            if isinstance(data, dict) and 'data' in data:
                game_data = data['data']
            else:
                game_data = data
            
            # íŒ€ ì•½ì–´ ë³€í™˜
            home_team_abbrev = self.NBA_TEAM_ABBREV.get(home_team_full, home_team_full)
            away_team_abbrev = self.NBA_TEAM_ABBREV.get(away_team_full, away_team_full)
            
            # UTC â†’ ET ë³€í™˜
            if commence_time_utc:
                utc_dt = datetime.fromisoformat(commence_time_utc.replace('Z', '+00:00'))
                et_dt = utc_dt.astimezone(self.eastern_tz)
                game_date = et_dt.strftime('%Y-%m-%d')
                commence_time_et = et_dt.strftime('%Y-%m-%d %H:%M:%S %Z')
            else:
                game_date = date_str
                commence_time_et = None
            
            # FanDuel alternate spreads ì¶”ì¶œ
            processed_spreads = []
            
            bookmakers = game_data.get('bookmakers', [])
            for bookmaker in bookmakers:
                if bookmaker.get('key') != 'fanduel':
                    continue
                
                markets = bookmaker.get('markets', [])
                for market in markets:
                    if market.get('key') != 'alternate_spreads':
                        continue
                    
                    outcomes = market.get('outcomes', [])
                    
                    for outcome in outcomes:
                        team_name = outcome.get('name', '')
                        spread_point = outcome.get('point')
                        odds = outcome.get('price')
                        
                        # ì›í•˜ëŠ” spread ë²”ìœ„ë§Œ í•„í„°ë§
                        if spread_point not in self.TARGET_SPREADS:
                            continue
                        
                        # í™ˆíŒ€ì¸ì§€ ì›ì •íŒ€ì¸ì§€ íŒë³„
                        is_home = (team_name == home_team_full)
                        team_abbrev = home_team_abbrev if is_home else away_team_abbrev
                        
                        spread_record = {
                            'game_id': game_id,
                            'date': game_date,
                            'commence_time_utc': commence_time_utc,
                            'commence_time_et': commence_time_et,
                            'home_team': home_team_abbrev,
                            'away_team': away_team_abbrev,
                            'home_team_full': home_team_full,
                            'away_team_full': away_team_full,
                            'team': team_abbrev,
                            'is_home': is_home,
                            'spread': spread_point,
                            'odds': odds,
                            'bookmaker': 'fanduel'
                        }
                        
                        processed_spreads.append(spread_record)
            
            return processed_spreads
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ Error fetching spreads for game {game_id}: {e}")
            return []
    
    def collect_alternate_spreads(self) -> str:
        """
        Alternate Spreads ìˆ˜ì§‘ ì‹¤í–‰
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´)
        """
        # ìˆ˜ì§‘í•  ë‚ ì§œ ìƒì„±
        target_dates = self.get_target_dates()
        
        if not target_dates:
            self.logger.info("âœ… No dates to collect. Master file is up to date.")
            return str(self.master_file) if self.master_file.exists() else ""
        
        self.logger.info("=" * 70)
        self.logger.info("ğŸ€ NBA Alternate Spreads Collection Started")
        self.logger.info("=" * 70)
        self.logger.info(f"ğŸ“… Target dates: {', '.join(target_dates)}")
        self.logger.info(f"ğŸ¯ Bookmaker: FanDuel")
        self.logger.info(f"ğŸ“Š Target spreads: {self.TARGET_SPREADS}")
        self.logger.info("=" * 70)
        
        all_spread_odds = []
        total_api_calls = 0
        successful_dates = 0
        failed_dates = 0
        
        for i, date_str in enumerate(target_dates, 1):
            self.logger.info(f"\n[{i}/{len(target_dates)}] Processing {date_str}...")
            
            # 1. í•´ë‹¹ ë‚ ì§œì˜ ê²½ê¸° ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            game_list = self.fetch_game_list_for_date(date_str)
            total_api_calls += 1
            
            if game_list is None:
                failed_dates += 1
                self.logger.warning(f"âŒ Failed to get game list for {date_str}")
                continue
            
            if not game_list:
                self.logger.info(f"ğŸ“­ No games found for {date_str}")
                successful_dates += 1
                continue
            
            self.logger.info(f"âœ… Found {len(game_list)} games")
            
            # 2. ê° ê²½ê¸°ì˜ alternate spreads ìˆ˜ì§‘
            date_spreads = 0
            for j, game in enumerate(game_list, 1):
                game_id = game['id']
                home_team_full = game['home_team']
                away_team_full = game['away_team']
                commence_time_utc = game['commence_time']
                
                home_abbrev = self.NBA_TEAM_ABBREV.get(home_team_full, home_team_full)
                away_abbrev = self.NBA_TEAM_ABBREV.get(away_team_full, away_team_full)
                
                self.logger.info(f"\n  [{j}/{len(game_list)}] {home_abbrev} vs {away_abbrev}")
                self.logger.info(f"      Game ID: {game_id}")
                
                processed_spreads = self.fetch_alternate_spreads_for_game(
                    game_id, date_str, home_team_full, away_team_full, commence_time_utc
                )
                total_api_calls += 1
                
                if processed_spreads:
                    all_spread_odds.extend(processed_spreads)
                    date_spreads += len(processed_spreads)
                    self.logger.info(f"      âœ… Collected {len(processed_spreads)} spread options")
                else:
                    self.logger.warning(f"      âš ï¸  No spreads found")
                
                # Rate limiting
                if j < len(game_list):
                    time.sleep(2)
            
            if date_spreads > 0:
                successful_dates += 1
                self.logger.info(f"\nâœ… {date_spreads} spread options collected for {date_str}")
            else:
                failed_dates += 1
            
            # ë‚ ì§œ ê°„ rate limiting
            if i < len(target_dates):
                self.logger.info("â³ Waiting 2 seconds...")
                time.sleep(2)
        
        # ê²°ê³¼ ì €ì¥
        if all_spread_odds or self.incremental:
            # âš ï¸ ì¤‘ìš”: ì €ì¥ ì „ ë¯¸ë˜ ë‚ ì§œ í•„í„°ë§ (ì–´ì œê¹Œì§€ë§Œ ìœ ì§€)
            today_et = datetime.now(self.eastern_tz).date()
            yesterday_et = today_et - timedelta(days=1)
            yesterday_str = yesterday_et.strftime('%Y-%m-%d')
            
            if self.incremental:
                # ì¦ë¶„ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
                existing_data = self.load_master_file()
                
                # ê¸°ì¡´ ë°ì´í„°ì—ì„œë„ ë¯¸ë˜ ë‚ ì§œ ì œê±°
                existing_data = [item for item in existing_data if item.get('date', '') <= yesterday_str]
                
                if all_spread_odds:
                    # ìƒˆ ë°ì´í„°ì—ì„œ ë¯¸ë˜ ë‚ ì§œ ì œê±°
                    all_spread_odds = [item for item in all_spread_odds if item.get('date', '') <= yesterday_str]
                    
                    # ìƒˆ ë°ì´í„° ì¶”ê°€
                    combined_data = existing_data + all_spread_odds
                    
                    # ì¤‘ë³µ ì œê±° (game_id + date + team + spread ê¸°ì¤€)
                    seen = set()
                    unique_data = []
                    for item in combined_data:
                        key = (item.get('game_id'), item.get('date'), 
                              item.get('team'), item.get('spread'))
                        if key not in seen:
                            seen.add(key)
                            unique_data.append(item)
                    
                    # ë‚ ì§œì™€ íŒ€ìœ¼ë¡œ ì •ë ¬
                    unique_data.sort(key=lambda x: (x['date'], x['home_team'], x['away_team'], x['team'], x['spread']))
                    
                    # ë§ˆìŠ¤í„° íŒŒì¼ ì €ì¥
                    with open(self.master_file, 'w', encoding='utf-8') as f:
                        json.dump(unique_data, f, indent=2, ensure_ascii=False)
                    
                    self.logger.info("\n" + "=" * 70)
                    self.logger.info("ğŸ‰ Incremental Update Completed!")
                    self.logger.info("=" * 70)
                    self.logger.info(f"ğŸ“Š New spread options collected: {len(all_spread_odds)}")
                    self.logger.info(f"ğŸ“š Total spread options in master: {len(unique_data)}")
                    self.logger.info(f"âœ… Successful dates: {successful_dates}/{len(target_dates)}")
                    self.logger.info(f"âŒ Failed dates: {failed_dates}/{len(target_dates)}")
                    self.logger.info(f"ğŸ”¢ Total API calls: {total_api_calls}")
                    self.logger.info(f"ğŸ’¾ Master file updated: {self.master_file}")
                    self.logger.info("=" * 70)
                    
                    return str(self.master_file)
                else:
                    # ìƒˆ ë°ì´í„° ì—†ì–´ë„ ê¸°ì¡´ ë°ì´í„°ì—ì„œ ë¯¸ë˜ ë‚ ì§œ ì œê±° í›„ ì €ì¥
                    if existing_data:
                        existing_data.sort(key=lambda x: (x['date'], x['home_team'], x['away_team'], x['team'], x['spread']))
                        with open(self.master_file, 'w', encoding='utf-8') as f:
                            json.dump(existing_data, f, indent=2, ensure_ascii=False)
                        self.logger.info(f"âœ… Master file cleaned (future dates removed): {len(existing_data)} records")
                    else:
                        self.logger.info("\nâœ… No new data to add. Master file unchanged.")
                    return str(self.master_file)
            else:
                # ì „ì²´ ìˆ˜ì§‘ ëª¨ë“œ: íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ + ë§ˆìŠ¤í„° íŒŒì¼ ì—…ë°ì´íŠ¸
                # ë¯¸ë˜ ë‚ ì§œ ì œê±°
                all_spread_odds = [item for item in all_spread_odds if item.get('date', '') <= yesterday_str]
                all_spread_odds.sort(key=lambda x: (x['date'], x['home_team'], x['away_team'], x['team'], x['spread']))
                
                # 1. íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ ì €ì¥ (ë°±ì—…ìš©)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = self.spreads_dir / f"nba_alternate_spreads_{timestamp}.json"
                
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(all_spread_odds, f, indent=2, ensure_ascii=False)
                
                # 2. ë§ˆìŠ¤í„° íŒŒì¼ ì—…ë°ì´íŠ¸
                with open(self.master_file, 'w', encoding='utf-8') as f:
                    json.dump(all_spread_odds, f, indent=2, ensure_ascii=False)
                
                # ìš”ì•½ ì¶œë ¥
                self.logger.info("\n" + "=" * 70)
                self.logger.info("ğŸ‰ Full Collection Completed!")
                self.logger.info("=" * 70)
                self.logger.info(f"ğŸ“Š Total spread options collected: {len(all_spread_odds)}")
                self.logger.info(f"âœ… Successful dates: {successful_dates}/{len(target_dates)}")
                self.logger.info(f"âŒ Failed dates: {failed_dates}/{len(target_dates)}")
                self.logger.info(f"ğŸ”¢ Total API calls: {total_api_calls}")
                self.logger.info(f"ğŸ’¾ Backup saved to: {backup_file}")
                self.logger.info(f"ğŸ’¾ Master file updated: {self.master_file}")
                self.logger.info("=" * 70)
                
                # í†µê³„
                unique_games = len(set((s['game_id'], s['date']) for s in all_spread_odds))
                avg_spreads = len(all_spread_odds) / unique_games if unique_games > 0 else 0
                self.logger.info(f"ğŸ€ Games processed: {unique_games}")
                self.logger.info(f"ğŸ“ˆ Avg spreads per game: {avg_spreads:.1f}")
                
                # Spread ë¶„í¬
                from collections import Counter
                spread_counts = Counter(s['spread'] for s in all_spread_odds)
                self.logger.info("\nğŸ“Š Spread distribution:")
                for spread in sorted(spread_counts.keys()):
                    self.logger.info(f"    {spread:+5.1f}: {spread_counts[spread]} options")
                self.logger.info("=" * 70)
                
                return str(self.master_file)
        else:
            self.logger.error("\n" + "=" * 70)
            self.logger.error("âŒ No spread data collected")
            self.logger.error("=" * 70)
            return ""


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # API í‚¤ ì„¤ì •
    API_KEY = "81fef80fc013d2c82c9a625ac1fca6b1"
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = NBAAlternateSpreadsCollector(
        api_key=API_KEY,
        days_back=2,  # ì¦ë¶„ ëª¨ë“œì—ì„œëŠ” ë¬´ì‹œë¨
        incremental=True  # âœ… ì¦ë¶„ ëª¨ë“œ (ë§ˆìŠ¤í„° íŒŒì¼ ì´ì–´ì„œ ìˆ˜ì§‘)
    )
    
    print("\n" + "=" * 70)
    print("ğŸ€ NBA Alternate Spreads Collector v2")
    print("=" * 70)
    print(f"ğŸ“… Mode: {'Incremental (new dates only)' if collector.incremental else f'Full scan (last {collector.days_back} days)'}")
    print(f"ğŸ¯ Target: FanDuel alternate spreads (Â±2.5 to Â±12.5, 22 options)")
    print(f"ğŸ“‚ Master file: data/alternate_spreads/nba_alternate_spreads_fanduel_master.json")
    print("=" * 70)
    
    # Spreads ìˆ˜ì§‘ ì‹¤í–‰
    output_file = collector.collect_alternate_spreads()
    
    if output_file:
        print("\nâœ… Success! Data saved to:")
        print(f"   {output_file}")
        print("\nğŸ’¡ Next steps:")
        print("   1. Set incremental=True for daily updates")
        print("   2. Run merge and analysis pipeline")
    else:
        print("\nâŒ Collection failed. Please check the logs above.")


if __name__ == "__main__":
    main()

